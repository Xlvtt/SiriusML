{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Поиск токенов ответа\n",
    "\n",
    "##### Описание решения\n",
    "Используем предложенный датасет sbersquad.\n",
    "О чудо! В таргете нам даны позиция начала и конца ответа, обучим модель предсказывать начало и конец ответа в вопросе, пользуясь тем фактом, что ответ гарантированно есть в контексте.\n",
    "Сведем к задаче классификации. Для каждого токена модель будет предсказывать вероятности того, что ответ начинается в этом токене и вероятности того, что ответ в нем заканчивается. Да, это еще одно очень сильное утверждение. \n",
    "Далее обо всем подробно!\n",
    "\n",
    "\n",
    "##### План работы:\n",
    "1. Выбор модели\n",
    "2. Подготовка данных\n",
    "3. Обучение\n",
    "4. Инференс\n",
    "5. Валидация\n",
    "6. Выводы"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b655186955089528"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Базовые импорты"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "347fd817cf83af8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T19:21:48.228589Z",
     "start_time": "2024-10-23T19:21:48.224819Z"
    }
   },
   "id": "62a828a2507c496a",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Выбор модели"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe304124b61c4c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используем модель rubert-tiny\n",
    "1. Обучался на русской википедии\n",
    "2. В качестве tokenizer использовался WordPiece на 30 000 токенов в словаре\n",
    "3. Учится сильно быстрее обычного rubert, так что отлично подойдет для обучения локально\n",
    "4. Это все еще старый добрый берт, отлично понимающий структуру языка"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3821df186ccb34d8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "DOC_STRIDE = 64\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 3\n",
    "MODEL_CHECKPOINT = \"cointegrated/rubert-tiny\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:45.963944Z",
     "start_time": "2024-10-23T18:56:45.959944Z"
    }
   },
   "id": "c3e2167aaa323338",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Скачаем словарь, чтобы создать токенизатор для модели"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42e67ab19fc67880"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subprocess.run([\"powershell\", \"-Command\", f\"wget https://huggingface.co/{MODEL_CHECKPOINT}/resolve/main/vocab.txt -OutFile vocab.txt\"], capture_output=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f21c2464b741850b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BertTokenizerFast(name_or_path='', vocab_size=29564, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast(\"vocab.txt\", do_lower_case=False, clean_up_tokenization_spaces=True, padding_side=\"right\")\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:46.534130Z",
     "start_time": "2024-10-23T18:56:46.478545Z"
    }
   },
   "id": "497c8c3e809ee087",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Подготовка данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84cb6f27ac3fdbb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузим данные с Hugging Face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f999b2c8e1cb112d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since kuznetsoffandrey/sberquad couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'sberquad' at C:\\Users\\Night\\.cache\\huggingface\\datasets\\kuznetsoffandrey___sberquad\\sberquad\\0.0.0\\deb870ee29b280657470f2c3851e9b23899fdbc3 (last modified on Tue Oct 22 14:52:11 2024).\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"kuznetsoffandrey/sberquad\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:45.878886Z",
     "start_time": "2024-10-23T18:56:45.772012Z"
    }
   },
   "id": "785ad50c7a990cf6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Убедимся, что данные подгрузились как надо."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93331dc53bcb3d33"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  В протерозойских отложениях органические остатки встречаются намного чаще, чем в архейских. Они представлены известковыми выделениями сине-зелёных водорослей, ходами червей, остатками кишечнополостных. Кроме известковых водорослей, к числу древнейших растительных остатков относятся скопления графито-углистого вещества, образовавшегося в результате разложения Corycium enigmaticum. В кремнистых сланцах железорудной формации Канады найдены нитевидные водоросли, грибные нити и формы, близкие современным кокколитофоридам. В железистых кварцитах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельности бактерий.\n",
      "Question:  чем представлены органические остатки?\n",
      "Answer:  {'text': ['известковыми выделениями сине-зелёных водорослей'], 'answer_start': [109]}\n"
     ]
    }
   ],
   "source": [
    "train_set = ds[\"train\"]\n",
    "valid_set = ds[\"validation\"]\n",
    "test_set = ds[\"test\"]\n",
    "\n",
    "print(\"Context: \", train_set[0][\"context\"])\n",
    "print(\"Question: \", train_set[0][\"question\"])\n",
    "print(\"Answer: \", train_set[0][\"answers\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:45.885017Z",
     "start_time": "2024-10-23T18:56:45.879707Z"
    }
   },
   "id": "a6295a8f417aa03c",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим, что ответы для всех датасетов определены однозначно"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95de8f5d194f714"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assert len(train_set.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)) == 0\n",
    "assert len(valid_set.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)) == 0\n",
    "assert len(test_set.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)) == 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:45.958934Z",
     "start_time": "2024-10-23T18:56:45.886023Z"
    }
   },
   "id": "6632f44bf08b549f",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Начнем готовить входные данные!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa0cacfd1217c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как бороться с текстами разной длины? Мы могли бы просто обрезать тексты, но тогда ответ может не попасть к вопросу.\n",
    "- Разобьем текст на k пересекающихся окон.\n",
    "- Если ответа там нет или он обрезан, положим start = end и пустую строку в качестве ответа\n",
    "- return_offsets_mapping нужен, чтобы возвращать индексы символов начала и конца каждого токена в параметр offset_mapping. В итоге в нем будут записаны для каждого токена его [l, r) индексы в исходном тексте"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d7ae20a5e26fbd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим на этот подход в действии"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee7fa61d447f674"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] чем представлены органические остатки? [SEP] В протерозойских отложениях органические остатки встречаются намного чаще, чем в ар [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] встречаются намного чаще, чем в архейских. Они представлены известковыми выделения [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] известковыми выделениями сине - зелёных водорослей, ход [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]елёных водорослей, ходами червей, остатками кишечнополос [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] остатками кишечнополостных. Кроме известковых водорослей, к [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]стковых водорослей, к числу древнейших растительных остатков относятся [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]ших растительных остатков относятся скопления графито - углистого вещества, образ [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] - углистого вещества, образовавшегося в результате разложения Corycium enig [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] разложения Corycium enigmaticum. В кремнистых сланцах желе [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]ремнистых сланцах железорудной формации Канады найдены нитевидные вод [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] Канады найдены нитевидные водоросли, грибные нити и формы, близкие сов [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] нити и формы, близкие современным кокколитофоридам. В желе [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]офоридам. В железистых кварцитах Северной Америки и Сибири обнару [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]тах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельно [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]е продукты жизнедеятельности бактерий. [SEP]\n"
     ]
    }
   ],
   "source": [
    "context = train_set[0][\"context\"]\n",
    "question = train_set[0][\"question\"]\n",
    "\n",
    "inputs : dict = tokenizer(question, context, max_length=32, truncation=\"only_second\", stride=8, return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:46.543938Z",
     "start_time": "2024-10-23T18:56:46.535138Z"
    }
   },
   "id": "5eb9e538e6b369df",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь остается дополнить текст паддингами.\n",
    "Создадим тот же tokenizer, но установим параметр padding=\"max_length\".\n",
    "Далее посмотрим, как они отображаются в attention_mask, offset_mapping и input_ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f20c46d384d347"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(0, 0), (0, 3), (4, 16), (17, 23), (23, 29), (30, 32), (32, 35), (35, 37), (37, 38), (0, 0), (0, 1), (2, 7), (7, 9), (9, 11), (11, 16), (17, 19), (19, 26), (26, 27), (28, 34), (34, 40), (41, 43), (43, 46), (46, 48), (49, 60), (61, 64), (64, 68), (69, 73), (73, 74), (75, 78), (79, 80), (81, 83), (0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] чем представлены органические остатки? [SEP]е продукты жизнедеятельности бактерий. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_samples = tokenizer(\n",
    "    train_set[0][\"question\"],\n",
    "    train_set[0][\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    max_length=32,\n",
    "    stride=8,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "print(tokenized_samples[\"attention_mask\"][-1])\n",
    "print(tokenized_samples[\"offset_mapping\"][0])\n",
    "tokenizer.decode(tokenized_samples[\"input_ids\"][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:56:46.585620Z",
     "start_time": "2024-10-23T18:56:46.571457Z"
    }
   },
   "id": "cc01c8b64ad0ba69",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Наконец напишем функцию для подготовки тренировочных и валидационных данных:\n",
    "1. Обрежем, западдим и токенизируем последовательности\n",
    "2. Запишем в start_positions индексы токенов начала ответа, а в end_positions - индексы токенов конца ответа"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb230a7d72e2d5f5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(45328, 6, 62033, 62033)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_train_data(examples: dict):  # в словарь маппится целый батч\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized_samples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )  # получает список текстов, возвращает список списков токенов\n",
    "    # все передаваемые на вход списки склеивает, получая один список вопрос _+ ответ\n",
    "    offset_mapping = tokenized_samples.pop(\"offset_mapping\") # токен - [l, r) позиция его в начальном тексте\n",
    "    overflow_to_sample_mapping = tokenized_samples.pop(\"overflow_to_sample_mapping\")  # в случае переполнения поймем, к какому начальному тексту относится кусок \n",
    "\n",
    "    answer_start_positions = [] # Индексы токенов начала ответа\n",
    "    answer_end_positions = [] # индексы токенов конца ответ\n",
    "    masked_offset_mapping = []\n",
    "    \n",
    "    # examples[\"answers\"][i][\"answer_start\"] - начало ответа для i-того текста\n",
    "    # examples[\"answers\"][i][\"answer_start\"] + len(examples[\"answers\"][i][\"text\"]) - конец i-того текста не включительно\n",
    "    # Если ответ поместился полностью, ставим\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = overflow_to_sample_mapping[i]\n",
    "        answer_start_index = examples[\"answers\"][sample_index][\"answer_start\"][0]\n",
    "        answer_end_index = answer_start_index + len(examples[\"answers\"][sample_index][\"text\"][0])\n",
    "        # ответ в [answer_start_index, answer_end_index]\n",
    "        cls_token_index = tokenized_samples[\"input_ids\"][sample_index].index(tokenizer.cls_token_id)  # на cls сгружаем все ненайденные ответы\n",
    "\n",
    "        pointer = 0\n",
    "        sequence_ids = tokenized_samples.sequence_ids(i) # так получаем список, который маппит склеенный текст в индексы переданных тексто\n",
    "        masked_offset_mapping.append([offset if sequence_ids[i] == 1 else None for i, offset in enumerate(offsets)])\n",
    "        \n",
    "        while pointer < len(sequence_ids) and sequence_ids[pointer] != 1: \n",
    "            pointer += 1\n",
    "        context_start = pointer\n",
    "        \n",
    "        while pointer < len(sequence_ids) and sequence_ids[pointer] == 1:\n",
    "            pointer += 1\n",
    "        context_end = pointer - 1\n",
    "        \n",
    "        if answer_end_index - 1 > offsets[context_end][1]: # ответ разорван и не входит в текст\n",
    "            answer_start_positions.append(cls_token_index)\n",
    "            answer_end_positions.append(cls_token_index)\n",
    "        else:\n",
    "            # ответ полностью входит в наш кусок\n",
    "            # индексы в offsets стоят по каждому тексту отдельно. Нам нужно стартовать с позиции старта контекста.\n",
    "            pointer = context_start\n",
    "            while pointer <= context_end and offsets[pointer][0] <= answer_start_index: # берем последний токен, начало которого <= индекс старта\n",
    "                # pointer-тый токен стоит в тексте на позиции [left_ind, right_ind)\n",
    "                pointer += 1\n",
    "            answer_start_positions.append(pointer - 1) # пушим индекс в склеенной последовательности токенов (в одном контексте это j - context_start + 1)\n",
    "            \n",
    "            pointer = context_end\n",
    "            while pointer >= context_start and offsets[pointer][1] >= answer_end_index: # берем с запасом, чтобы токен точно вошел\n",
    "                # pointer-тый токен стоит в тексте на позиции [left_ind, right_ind)\n",
    "                pointer -= 1\n",
    "            answer_end_positions.append(pointer + 1) # пушим индекс в склеенной последовательности токенов (в одном контексте это j - context_start + 1)\n",
    "            \n",
    "    tokenized_samples[\"start_positions\"] = answer_start_positions\n",
    "    tokenized_samples[\"end_positions\"] = answer_end_positions\n",
    "    tokenized_samples[\"offset_mapping\"] = masked_offset_mapping\n",
    "    \n",
    "    return tokenized_samples\n",
    "\n",
    "res = prepare_train_data(train_set[0:-1]) \n",
    "answer_start = res[\"start_positions\"]\n",
    "answer_end = res[\"end_positions\"]\n",
    "len(train_set), len(res), len(answer_start), len(answer_end)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:12.347613Z",
     "start_time": "2024-10-23T18:56:46.586623Z"
    }
   },
   "id": "c8f0526509200ab1",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запустим функцию на трейне и валидации"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e8e3fca1861afec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_train = train_set.map(prepare_train_data, batched=True, remove_columns=train_set.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:12.518830Z",
     "start_time": "2024-10-23T18:57:12.351648Z"
    }
   },
   "id": "41c7b311156711ce",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_valid = valid_set.map(prepare_train_data, batched=True, remove_columns=valid_set.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:12.634943Z",
     "start_time": "2024-10-23T18:57:12.526807Z"
    }
   },
   "id": "f8b444cc8f0d8d30",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь создадим функцию для обработки теста и подготовки данных к инференсу.\n",
    "Она отличается тем, что нам не нужно создавать таргет, но нужно задать айдишники и маску для восстановления ответа"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1af188026c3cd59a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_test_data(examples: dict):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized_samples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    id_list = []\n",
    "    masked_offset_mapping = []\n",
    "    overflow_to_sample_mapping = tokenized_samples.pop(\"overflow_to_sample_mapping\")  # в случае переполнения поймем, к какому начальному тексту относится кусок \n",
    "    \n",
    "    for i in range(len(tokenized_samples[\"input_ids\"])):\n",
    "        sample_index = overflow_to_sample_mapping[i]  # добавляем поле айдишника\n",
    "        id_list.append(examples[\"id\"][sample_index])\n",
    "        \n",
    "        sequence_ids = tokenized_samples.sequence_ids(i)\n",
    "        offsets = tokenized_samples[\"offset_mapping\"][i]\n",
    "        masked_offset_mapping.append([offset if sequence_ids[index] == 1 else None for index, offset in enumerate(offsets)])  # Маскируем все оффсеты не из контекста\n",
    "    \n",
    "    tokenized_samples[\"id\"] = id_list\n",
    "    tokenized_samples[\"offset_mapping\"] = masked_offset_mapping\n",
    "    return tokenized_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:12.648562Z",
     "start_time": "2024-10-23T18:57:12.636963Z"
    }
   },
   "id": "261ba5c9a040e22",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запустим функцию на тесте"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17569e0446363ca9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_test = test_set.map(prepare_test_data, batched=True, remove_columns=test_set.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:12.842164Z",
     "start_time": "2024-10-23T18:57:12.653301Z"
    }
   },
   "id": "c3f4996820521e53",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подготовка данных завершена!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2db17b1685be6075"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Обучение"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b512b589a8e641d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузим модель, выбранную ранее"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f9ecea523d4e88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "baseline = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:18.332814Z",
     "start_time": "2024-10-23T18:57:12.843269Z"
    }
   },
   "id": "a0297cf3e6ccf655",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запустим обучение с помощью внутреннего класса Trainer библиотеки transformers.\n",
    "Таргет автоматически берется из колонок с названиями start_positions и end_positions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d2edaf3e28b460c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:19.235113Z",
     "start_time": "2024-10-23T18:57:19.169431Z"
    }
   },
   "id": "6fa45d18ac335b7f",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:19.241621Z",
     "start_time": "2024-10-23T18:57:19.236525Z"
    }
   },
   "id": "1adfa85bb0be7037",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    baseline,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:19.525426Z",
     "start_time": "2024-10-23T18:57:19.244633Z"
    }
   },
   "id": "938a9f364b446c25",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучим модель, если ее еще не делали этого.\n",
    "Загрузим модель, если она уже была обучена."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e46133bbc7f033"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:19.603339Z",
     "start_time": "2024-10-23T18:57:19.543205Z"
    }
   },
   "id": "f511ac6fe9f51be3",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = Path(f\"./{model_name}\")\n",
    "if not model_path.exists():\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(model_path))\n",
    "    model = trainer.model\n",
    "else:\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(str(model_path))\n",
    "    trainer.model = model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:19.709688Z",
     "start_time": "2024-10-23T18:57:19.607005Z"
    }
   },
   "id": "313eb4db9b3a5b64",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим, в каком формате модель выдает данные."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c7248c5f22bbde2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
      "output keys:  odict_keys(['loss', 'start_logits', 'end_logits'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    print(\"batch keys:\", batch.keys())\n",
    "    break\n",
    "batch = {key: value.to(trainer.args.device) for key, value in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = model(**batch)  # Ждет те же данные, что и для обучения\n",
    "print(\"output keys: \", output.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:57:21.903573Z",
     "start_time": "2024-10-23T18:57:19.710704Z"
    }
   },
   "id": "c0b55988b48252b3",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Инференс"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80bc537210d37e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Используем сильное утверждение - будем считать токены начала и конца независимыми. Тогда вероятность ответа можно оценить как их произведение\n",
    "2. Отберем n_best_size лучших токенов начала и конца. Так получим $n\\_best\\_size^2$ лучших пар.\n",
    "3. Выбросим из них те, где конец находится раньше начала, а также число токенов в которых не соответствует диапазону [min_answer_len, max_answer_len]\n",
    "4. В качестве ответа выберем пару с высочайшим скором"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a5ba6997eff3e46"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1199.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 1.9342454671859741,\n  'text': 'Многоклеточный организм — внесистематическая категория живых организмов, тело которых состоит из многих клеток, большая часть которых (кроме стволовых, например, клеток камбия у растений) дифференцированы, то есть различаются по строению и выполняемым функциям',\n  'id': 18009},\n {'score': 3.1041817665100098,\n  'text': 'на генеративные и соматические.',\n  'id': 54494},\n {'score': 2.3413610458374023, 'text': 'соматические', 'id': 56411}]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(data: dict | datasets.Dataset, n_best_size=20, max_answer_len=256, min_answer_len=0) -> str:\n",
    "    \"\"\"\n",
    "    :param data: словарь формата теста (поля question, context и id)\n",
    "    :param n_best_size: сколько лучших значений логитов используется для поиска ответа\n",
    "    :param max_answer_len: не рассматриваем ответы с большей длиной даже если у них высокий скор\n",
    "    :param min_answer_len: аналогично не рассматриваем меньшую длину\n",
    "    :return: ответ на вопрос\n",
    "    \"\"\"\n",
    "    if isinstance(data, datasets.Dataset):\n",
    "        tokenized_data = data.map(prepare_test_data, batched=True, remove_columns=data.column_names)\n",
    "    else:\n",
    "        tokenized_data = prepare_test_data(data) \n",
    "    preds = model(\n",
    "        input_ids=torch.tensor(tokenized_data[\"input_ids\"], dtype=torch.int64),\n",
    "        attention_mask=torch.tensor(tokenized_data[\"attention_mask\"], dtype=torch.int64)\n",
    "    )\n",
    "    get_pieces_index = {} # словарь id существующего текста: индексы всех его кусков после токенизации\n",
    "    for i, sample_index in enumerate(tokenized_data[\"id\"]):\n",
    "        if sample_index not in get_pieces_index:\n",
    "            get_pieces_index[sample_index] = []\n",
    "        get_pieces_index[sample_index].append(i)\n",
    "    \n",
    "    total_answers = []\n",
    "    for sample_index, sample in enumerate(tqdm(data[\"id\"])):\n",
    "        context = data[\"context\"][sample_index] # текущий контекст\n",
    "        \n",
    "        valid_answers = []  # ответы для одного контекста\n",
    "        for piece_index in get_pieces_index[sample]: # перебираем все куски одного контекста, собираем ответы для каждого\n",
    "            start_logits = preds.start_logits[piece_index].detach().numpy() \n",
    "            end_logits = preds.end_logits[piece_index].detach().numpy()\n",
    "            offset_mapping = tokenized_data[\"offset_mapping\"][piece_index]\n",
    "            \n",
    "            best_start_tokens_ids = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            best_end_tokens_ids = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            \n",
    "            for start_token in best_start_tokens_ids:\n",
    "                for end_token in best_end_tokens_ids:\n",
    "                    if offset_mapping[start_token] is None or offset_mapping[end_token] is None:\n",
    "                        continue\n",
    "                    if (\n",
    "                            start_token > end_token or \n",
    "                            end_token - start_token + 1 > max_answer_len or \n",
    "                            end_token - start_token + 1 < min_answer_len\n",
    "                    ):\n",
    "                        continue\n",
    "                    valid_answers.append({\n",
    "                            \"score\": start_logits[start_token].item() + end_logits[end_token].item(),\n",
    "                            \"text\": context[offset_mapping[start_token][0] : offset_mapping[end_token][1]],\n",
    "                            \"id\": sample\n",
    "                        })\n",
    "            if not valid_answers:\n",
    "                valid_answers.append({\"score\": 0.0, \"text\": \"\", \"id\": sample})\n",
    "                \n",
    "        total_answers.append(sorted(valid_answers, key=lambda x: -x[\"score\"])[0]) # для всего контекста берем лучший по всем кусочкам\n",
    "    \n",
    "    return total_answers\n",
    "    \n",
    "some_samples = test_set[list(range(3))]\n",
    "inference(some_samples, min_answer_len=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T07:50:32.327535Z",
     "start_time": "2024-10-24T07:50:32.205007Z"
    }
   },
   "id": "b6f33c8f91e2940",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запустим инференс на своих данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "623ab817879a42b7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 938.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 3.2535845041275024, 'text': 'Никите Ляпину 20 лет', 'id': 0}]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference({\"question\": [\"Сколько Никите Ляпину годиков?\"], \"context\": [\"Никите Ляпину 20 лет\"], \"id\": [0]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T07:50:45.590588Z",
     "start_time": "2024-10-24T07:50:45.535452Z"
    }
   },
   "id": "d55bb5355f2a5e0a",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Валидация"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b12edeb56fa600b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используем метрику squad из библиотеки evaluate, чтобы посчитать F1 между эталонными ответами и ответами нашей модели"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9887246bdad7d44"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T18:58:48.410749Z",
     "start_time": "2024-09-30T18:58:46.773717Z"
    }
   },
   "id": "56f8b1784127c0f9",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preds_count = 1000\n",
    "predictions = inference(valid_set[:preds_count])\n",
    "formatted_predictions = [{\"id\": str(predictions[i][\"id\"]), \"prediction_text\": predictions[i][\"text\"]} for i in range(len(predictions))]\n",
    "references = [{\"id\": str(valid_set[i][\"id\"]), \"answers\": valid_set[i][\"answers\"]} for i in range(len(predictions))]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-09-30T20:18:53.810937Z"
    }
   },
   "id": "c288f6b99bbd6226",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'exact_match': 1.2, 'f1': 8.450913145631143}"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=formatted_predictions, references=references)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T20:16:42.937811Z",
     "start_time": "2024-09-30T20:16:41.813261Z"
    }
   },
   "id": "57bf68c94aca59fa",
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "source": [
    "Какой чудесный f1-score)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de6f1d5947e0b848"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Выводы и зоны роста"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8637d39f90a08d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Главные выводы:\n",
    "\n",
    "Качество ответов оставляет желать лучшего.\n",
    "Без ограничения минимального числа токенов в ответе модель выдает очень короткие бессмысленные ответы.\n",
    "Есть варианты, почему так происходит:\n",
    "1. Ошибка в подготовке данных (неправильно выставляется таргет)\n",
    "2. Ошибка в функции инференса (у 7% текстов в ответах стоит 0:0, что происходит, если текст дробится на части. Тогда у каждого куска свое множества исходов, а ответ мы выбираем среди всех таких кусков. Может оказаться так, что ответ выбран из того куска текста, в котором он на самом деле не содержится, так как вероятности никак не нормируются)\n",
    "3. Слабая модель и ошибки обучения. Здесь можно было сделать warm-up, подобрать learning rate или дообучить модель побольше. Неплохо бы строить графики в процессе обучения, используя torch (но это я уже делал после дедлайна)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eb60d45cacdaff7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Идеи по улучшению:\n",
    "Заметим, что предположение о том, что ответ уже есть контексте, довольно сильное. Можно было бы приложить такое решение к поиску по странице, но интереснее, когда вопрос задается абстрактно.\n",
    "Есть идея построить базу знаний, в которой некий алгоритм будет отбирать самые релевантные к запросу страницы, а уже потом предложенная здесь модель - искать среди них самый вероятный ответ"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dec5e4ae47209e6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
