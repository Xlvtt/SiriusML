{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Поиск токенов ответа\n",
    "\n",
    "##### Описание решения\n",
    "В таргете даны позиция начала и конца ответа, попробуем научить модель выделять эти позиции. \n",
    "Сведем к задаче классификации. Для каждого токена модель будет предсказывать вероятность начала ответа в нем и вероятности конца ответа в нем.\n",
    "Модель выдает вектор вероятностей размерности длины последовательности. Далее нужно замаскировать позиции, соответствующие токенам вопроса и служебным токенам, после чего прогнать через софтмакс.\n",
    "\n",
    "\n",
    "##### Обучение\n",
    "Пока идея - приделать две независимых головы для начала и для конца и учить модель на сумму их лоссов\n",
    "\n",
    "\n",
    "##### Инференс\n",
    "Вычислим вероятности всех пар, где start_pos <= end_pos, а из них выберем самую вероятную (хотя считать веротяности независимыми странно)\n",
    "Можно загрузить в бота текст для поиска ответа отдельной командой, а далее задавать к тексту вопросы."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b655186955089528"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import BertTokenizer, BertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:09.916635Z",
     "start_time": "2024-09-30T13:09:09.892851Z"
    }
   },
   "id": "62a828a2507c496a",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ds = load_dataset(\"kuznetsoffandrey/sberquad\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:23.035876Z",
     "start_time": "2024-09-30T13:09:16.580072Z"
    }
   },
   "id": "785ad50c7a990cf6",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  В протерозойских отложениях органические остатки встречаются намного чаще, чем в архейских. Они представлены известковыми выделениями сине-зелёных водорослей, ходами червей, остатками кишечнополостных. Кроме известковых водорослей, к числу древнейших растительных остатков относятся скопления графито-углистого вещества, образовавшегося в результате разложения Corycium enigmaticum. В кремнистых сланцах железорудной формации Канады найдены нитевидные водоросли, грибные нити и формы, близкие современным кокколитофоридам. В железистых кварцитах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельности бактерий.\n",
      "Question:  чем представлены органические остатки?\n",
      "Answer:  {'text': ['известковыми выделениями сине-зелёных водорослей'], 'answer_start': [109]}\n"
     ]
    }
   ],
   "source": [
    "train_set = ds[\"train\"]\n",
    "valid_set = ds[\"validation\"]\n",
    "test_set = ds[\"test\"]\n",
    "\n",
    "print(\"Context: \", train_set[0][\"context\"])\n",
    "print(\"Question: \", train_set[0][\"question\"])\n",
    "print(\"Answer: \", train_set[0][\"answers\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:23.089151Z",
     "start_time": "2024-09-30T13:09:23.038280Z"
    }
   },
   "id": "a6295a8f417aa03c",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим, что ответы для всех датасетов определены однозначно"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95de8f5d194f714"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assert len(train_set.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)) == 0\n",
    "assert len(valid_set.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)) == 0\n",
    "assert len(test_set.filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)) == 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:23.152869Z",
     "start_time": "2024-09-30T13:09:23.091211Z"
    }
   },
   "id": "6632f44bf08b549f",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "Начнем с модели rubert-tiny\n",
    "1. Обучался на русской википедии\n",
    "2. качестве tokenizer использовался WordPiece на 30к токенов в словаре"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3821df186ccb34d8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "DOC_STRIDE = 64\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 3\n",
    "MODEL_CHECKPOINT = \"cointegrated/rubert-tiny\" #\"DeepPavlov/rubert-base-cased\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:23.158269Z",
     "start_time": "2024-09-30T13:09:23.154421Z"
    }
   },
   "id": "c3e2167aaa323338",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "Скачаем словарь для модели, чтобы создать токенизатор"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "645aa9d13cb68960"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "CompletedProcess(args=['powershell', '-Command', 'wget https://huggingface.co/cointegrated/rubert-tiny/resolve/main/vocab.txt -OutFile vocab.txt'], returncode=0, stdout=b'', stderr=b'')"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"powershell\", \"-Command\", f\"wget https://huggingface.co/{MODEL_CHECKPOINT}/resolve/main/vocab.txt -OutFile vocab.txt\"], capture_output=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:23.889899Z",
     "start_time": "2024-09-30T13:09:23.159358Z"
    }
   },
   "id": "c6d0a9e39333ddaf",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BertTokenizerFast(name_or_path='', vocab_size=29564, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast(\"vocab.txt\", do_lower_case=False, clean_up_tokenization_spaces=True, padding_side=\"right\")\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:23.974174Z",
     "start_time": "2024-09-30T13:09:23.890932Z"
    }
   },
   "id": "497c8c3e809ee087",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Мы можем обрезать тексты, но тогда ответ можем не попасть к вопросу. Разобьем текст на k пересекающихся окон.\n",
    "- Если ответа там нет или он обрезан, положим start=end и пустую строку в качестве ответа. Для этого используем параметр return_overflowing_tokens.\n",
    "- return_offsets_mapping нужен, чтобы возвращать индексы начала и конца каждого токена в словарь в параметр offset_mapping, в нем для каждого токена его [l, r) индексы в исходном тексте"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d7ae20a5e26fbd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим на этот подход в действии"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee7fa61d447f674"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] чем представлены органические остатки? [SEP] В протерозойских отложениях органические остатки встречаются намного чаще, чем в ар [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] встречаются намного чаще, чем в архейских. Они представлены известковыми выделения [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] известковыми выделениями сине - зелёных водорослей, ход [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]елёных водорослей, ходами червей, остатками кишечнополос [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] остатками кишечнополостных. Кроме известковых водорослей, к [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]стковых водорослей, к числу древнейших растительных остатков относятся [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]ших растительных остатков относятся скопления графито - углистого вещества, образ [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] - углистого вещества, образовавшегося в результате разложения Corycium enig [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] разложения Corycium enigmaticum. В кремнистых сланцах желе [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]ремнистых сланцах железорудной формации Канады найдены нитевидные вод [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] Канады найдены нитевидные водоросли, грибные нити и формы, близкие сов [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP] нити и формы, близкие современным кокколитофоридам. В желе [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]офоридам. В железистых кварцитах Северной Америки и Сибири обнару [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]тах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельно [SEP]\n",
      "[CLS] чем представлены органические остатки? [SEP]е продукты жизнедеятельности бактерий. [SEP]\n"
     ]
    }
   ],
   "source": [
    "context = train_set[0][\"context\"]\n",
    "question = train_set[0][\"question\"]\n",
    "\n",
    "inputs : dict = tokenizer(question, context, max_length=32, truncation=\"only_second\", stride=8, return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:24.004543Z",
     "start_time": "2024-09-30T13:09:23.975397Z"
    }
   },
   "id": "5eb9e538e6b369df",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"][0]) == len(inputs[\"offset_mapping\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:24.011957Z",
     "start_time": "2024-09-30T13:09:24.005602Z"
    }
   },
   "id": "400eb7e98b97ee17",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['id', 'title', 'context', 'question', 'answers']"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.column_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:24.020063Z",
     "start_time": "2024-09-30T13:09:24.014202Z"
    }
   },
   "id": "eab025d14fbe8712",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 62310,\n 'title': 'SberChallenge',\n 'context': 'В протерозойских отложениях органические остатки встречаются намного чаще, чем в архейских. Они представлены известковыми выделениями сине-зелёных водорослей, ходами червей, остатками кишечнополостных. Кроме известковых водорослей, к числу древнейших растительных остатков относятся скопления графито-углистого вещества, образовавшегося в результате разложения Corycium enigmaticum. В кремнистых сланцах железорудной формации Канады найдены нитевидные водоросли, грибные нити и формы, близкие современным кокколитофоридам. В железистых кварцитах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельности бактерий.',\n 'question': 'чем представлены органические остатки?',\n 'answers': {'text': ['известковыми выделениями сине-зелёных водорослей'],\n  'answer_start': [109]}}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:24.029336Z",
     "start_time": "2024-09-30T13:09:24.021164Z"
    }
   },
   "id": "3d8d3cf3f7c0e5de",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "Дополним паддингами и посмотрим, как они отображаются в аутпуте"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f20c46d384d347"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(0, 0), (0, 3), (4, 16), (17, 23), (23, 29), (30, 32), (32, 35), (35, 37), (37, 38), (0, 0), (0, 1), (2, 7), (7, 9), (9, 11), (11, 16), (17, 19), (19, 26), (26, 27), (28, 34), (34, 40), (41, 43), (43, 46), (46, 48), (49, 60), (61, 64), (64, 68), (69, 73), (73, 74), (75, 78), (79, 80), (81, 83), (0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] чем представлены органические остатки? [SEP]е продукты жизнедеятельности бактерий. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_samples = tokenizer(\n",
    "    train_set[0][\"question\"],\n",
    "    train_set[0][\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    max_length=32,\n",
    "    stride=8,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "print(tokenized_samples[\"attention_mask\"][-1])\n",
    "print(tokenized_samples[\"offset_mapping\"][0])\n",
    "tokenizer.decode(tokenized_samples[\"input_ids\"][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:09:24.051719Z",
     "start_time": "2024-09-30T13:09:24.031041Z"
    }
   },
   "id": "cc01c8b64ad0ba69",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь можем писать функцию препроцессинга трейна:\n",
    "1. Обрежем, западдим и токенизируем последовательности\n",
    "2. Запишем в таргет индекс токена-начала и индекс токена-конца"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb230a7d72e2d5f5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(45328, 6, 62033, 62033)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_train_data(examples: dict):  # в словарь маппится целый батч\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized_samples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )  # получает список текстов, возвращает список списков токенов\n",
    "    # все передаваемые на вход списки склеивает, получая один список вопрос _+ ответ\n",
    "    offset_mapping = tokenized_samples.pop(\"offset_mapping\") # токен - [l, r) позиция его в начальном тексте\n",
    "    overflow_to_sample_mapping = tokenized_samples.pop(\"overflow_to_sample_mapping\")  # в случае переполнения поймем, к какому начальному тексту относится кусок \n",
    "\n",
    "    answer_start_positions = [] # Индексы токенов начала ответа\n",
    "    answer_end_positions = [] # индексы токенов конца ответ\n",
    "    masked_offset_mapping = []\n",
    "    \n",
    "    # examples[\"answers\"][i][\"answer_start\"] - начало ответа для i-того текста\n",
    "    # examples[\"answers\"][i][\"answer_start\"] + len(examples[\"answers\"][i][\"text\"]) - конец i-того текста не включительно\n",
    "    # Если ответ поместился полностью, ставим\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = overflow_to_sample_mapping[i]\n",
    "        answer_start_index = examples[\"answers\"][sample_index][\"answer_start\"][0]\n",
    "        answer_end_index = answer_start_index + len(examples[\"answers\"][sample_index][\"text\"][0])\n",
    "        # ответ в [answer_start_index, answer_end_index]\n",
    "        cls_token_index = tokenized_samples[\"input_ids\"][sample_index].index(tokenizer.cls_token_id)  # на cls сгружаем все ненайденные ответы\n",
    "\n",
    "        pointer = 0\n",
    "        sequence_ids = tokenized_samples.sequence_ids(i) # так получаем список, который маппит склеенный текст в индексы переданных тексто\n",
    "        masked_offset_mapping.append([offset if sequence_ids[i] == 1 else None for i, offset in enumerate(offsets)])\n",
    "        \n",
    "        while pointer < len(sequence_ids) and sequence_ids[pointer] != 1: \n",
    "            pointer += 1\n",
    "        context_start = pointer\n",
    "        \n",
    "        while pointer < len(sequence_ids) and sequence_ids[pointer] == 1:\n",
    "            pointer += 1\n",
    "        context_end = pointer - 1\n",
    "        \n",
    "        if answer_end_index - 1 > offsets[context_end][1]: # ответ разорван и не входит в текст\n",
    "            answer_start_positions.append(cls_token_index)\n",
    "            answer_end_positions.append(cls_token_index)\n",
    "        else:\n",
    "            # ответ полностью входит в наш кусок\n",
    "            # индексы в offsets стоят по каждому тексту отдельно. Нам нужно стартовать с позиции старта контекста.\n",
    "            pointer = context_start\n",
    "            while pointer <= context_end and offsets[pointer][0] <= answer_start_index: # берем последний токен, начало которого <= индекс старта\n",
    "                # pointer-тый токен стоит в тексте на позиции [left_ind, right_ind)\n",
    "                pointer += 1\n",
    "            answer_start_positions.append(pointer - 1) # пушим индекс в склеенной последовательности токенов (в одном контексте это j - context_start + 1)\n",
    "            \n",
    "            pointer = context_end\n",
    "            while pointer >= context_start and offsets[pointer][1] >= answer_end_index: # берем с запасом, чтобы токен точно вошел\n",
    "                # pointer-тый токен стоит в тексте на позиции [left_ind, right_ind)\n",
    "                pointer -= 1\n",
    "            answer_end_positions.append(pointer + 1) # пушим индекс в склеенной последовательности токенов (в одном контексте это j - context_start + 1)\n",
    "            \n",
    "    tokenized_samples[\"start_positions\"] = answer_start_positions\n",
    "    tokenized_samples[\"end_positions\"] = answer_end_positions\n",
    "    tokenized_samples[\"offset_mapping\"] = masked_offset_mapping\n",
    "    \n",
    "    return tokenized_samples\n",
    "\n",
    "res = prepare_train_data(train_set[0:-1]) \n",
    "answer_start = res[\"start_positions\"]\n",
    "answer_end = res[\"end_positions\"]\n",
    "len(train_set), len(res), len(answer_start), len(answer_end)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:23.643625Z",
     "start_time": "2024-09-30T13:09:24.053846Z"
    }
   },
   "id": "c8f0526509200ab1",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_train = train_set.map(prepare_train_data, batched=True, remove_columns=train_set.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:23.928753Z",
     "start_time": "2024-09-30T13:11:23.650155Z"
    }
   },
   "id": "41c7b311156711ce",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_valid = valid_set.map(prepare_train_data, batched=True, remove_columns=valid_set.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.017663Z",
     "start_time": "2024-09-30T13:11:23.929816Z"
    }
   },
   "id": "f8b444cc8f0d8d30",
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь сделаем функцию для обработки теста и подготовки данных к инференсу.\n",
    "Она отличается тем, что нам не нужно создавать таргет, но нужно задать айдишники и маску для восстановления ответа"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1af188026c3cd59a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_test_data(examples: dict):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized_samples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    id_list = []\n",
    "    masked_offset_mapping = []\n",
    "    overflow_to_sample_mapping = tokenized_samples.pop(\"overflow_to_sample_mapping\")  # в случае переполнения поймем, к какому начальному тексту относится кусок \n",
    "    \n",
    "    for i in range(len(tokenized_samples[\"input_ids\"])):\n",
    "        sample_index = overflow_to_sample_mapping[i]  # добавляем поле айдишника\n",
    "        id_list.append(examples[\"id\"][sample_index])\n",
    "        \n",
    "        sequence_ids = tokenized_samples.sequence_ids(i)\n",
    "        offsets = tokenized_samples[\"offset_mapping\"][i]\n",
    "        masked_offset_mapping.append([offset if sequence_ids[index] == 1 else None for index, offset in enumerate(offsets)])  # Маскируем все оффсеты не из контекста\n",
    "    \n",
    "    tokenized_samples[\"id\"] = id_list\n",
    "    tokenized_samples[\"offset_mapping\"] = masked_offset_mapping\n",
    "    return tokenized_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.037806Z",
     "start_time": "2024-09-30T13:11:24.021710Z"
    }
   },
   "id": "261ba5c9a040e22",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_test = test_set.map(prepare_test_data, batched=True, remove_columns=test_set.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.426487Z",
     "start_time": "2024-09-30T13:11:24.039850Z"
    }
   },
   "id": "c3f4996820521e53",
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "Из колонок остаются поля с таргетом и сами данные"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17569e0446363ca9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Определим наконец нашу модель"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b512b589a8e641d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.435986Z",
     "start_time": "2024-09-30T13:11:24.427493Z"
    }
   },
   "id": "a0297cf3e6ccf655",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "baseline = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.746220Z",
     "start_time": "2024-09-30T13:11:24.436991Z"
    }
   },
   "id": "9c563c071a5718e4",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ну пусть класс AutoModelForQuestionAnswering знает, как проводить обучение\n",
    "Но откуда она знает, в каких полях датасета у меня лежит таргет, например?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d2edaf3e28b460c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.786907Z",
     "start_time": "2024-09-30T13:11:24.749228Z"
    }
   },
   "id": "6fa45d18ac335b7f",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.794065Z",
     "start_time": "2024-09-30T13:11:24.788912Z"
    }
   },
   "id": "1adfa85bb0be7037",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    baseline,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.926377Z",
     "start_time": "2024-09-30T13:11:24.800842Z"
    }
   },
   "id": "938a9f364b446c25",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions', 'offset_mapping'],\n    num_rows: 62035\n})"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.934093Z",
     "start_time": "2024-09-30T13:11:24.927386Z"
    }
   },
   "id": "67b0998757e89763",
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "source": [
    "Класс Trainer определяет поле для таргета по модели.\n",
    "В датасете может быть записано что угодно, но главное, чтобы был весь набор ожидаемых ключей.\n",
    "В нашем случае это таргет в start_positions и end_positions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f99a7e18516dbd9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучим / загрузим модель"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e46133bbc7f033"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:24.947232Z",
     "start_time": "2024-09-30T13:11:24.936100Z"
    }
   },
   "id": "f511ac6fe9f51be3",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = Path(f\"./{model_name}\")\n",
    "if not model_path.exists():\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(model_path))\n",
    "    model = trainer.model\n",
    "else:\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(str(model_path))\n",
    "    trainer.model = model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:25.037938Z",
     "start_time": "2024-09-30T13:11:24.949815Z"
    }
   },
   "id": "313eb4db9b3a5b64",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
      "output keys:  odict_keys(['loss', 'start_logits', 'end_logits'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    print(\"batch keys:\", batch.keys())\n",
    "    break\n",
    "batch = {key: value.to(trainer.args.device) for key, value in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = model(**batch)  # Ждет те же данные, что и для обучения\n",
    "print(\"output keys: \", output.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T13:11:26.319006Z",
     "start_time": "2024-09-30T13:11:25.040007Z"
    }
   },
   "id": "c0b55988b48252b3",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция для инференса будет работать по описанной в начале схеме. Выбираем несколько наиболее вероятных позиций начала ответа и конца, скорим все пары и берем лучшие"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a94ed86905063649"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 657.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 5.860038161277771, 'text': 'ние', 'id': 18009},\n {'score': 6.299153089523315, 'text': 'на', 'id': 54494},\n {'score': 6.293945789337158, 'text': 'со', 'id': 56411}]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(data: dict | datasets.Dataset, n_best_size=20, max_answer_len=256, min_answer_len=0) -> str:\n",
    "    \"\"\"\n",
    "    :param data: словарь формата теста (поля question, context и id)\n",
    "    :param n_best_size: сколько лучших значений логитов используется для поиска ответа\n",
    "    :param max_answer_len: не рассматриваем ответы с большей длиной даже если у них высокий скор\n",
    "    :param min_answer_len: аналогично не рассматриваем меньшую длину\n",
    "    :return: ответ на вопрос\n",
    "    \"\"\"\n",
    "    if isinstance(data, datasets.Dataset):\n",
    "        tokenized_data = data.map(prepare_test_data, batched=True, remove_columns=data.column_names)\n",
    "    else:\n",
    "        tokenized_data = prepare_test_data(data) \n",
    "    preds = model(\n",
    "        input_ids=torch.tensor(tokenized_data[\"input_ids\"], dtype=torch.int64),\n",
    "        attention_mask=torch.tensor(tokenized_data[\"attention_mask\"], dtype=torch.int64)\n",
    "    )\n",
    "    get_pieces_index = {} # словарь id существующего текста: индексы всех его кусков после токенизации\n",
    "    for i, sample_index in enumerate(tokenized_data[\"id\"]):\n",
    "        if sample_index not in get_pieces_index:\n",
    "            get_pieces_index[sample_index] = []\n",
    "        get_pieces_index[sample_index].append(i)\n",
    "    \n",
    "    total_answers = []\n",
    "    for sample_index, sample in enumerate(tqdm(data[\"id\"])):\n",
    "        context = data[\"context\"][sample_index] # текущий контекст\n",
    "        \n",
    "        valid_answers = []  # ответы для одного контекста\n",
    "        for piece_index in get_pieces_index[sample]: # перебираем все куски одного контекста, собираем ответы для каждого\n",
    "            start_logits = preds.start_logits[piece_index].detach().numpy() \n",
    "            end_logits = preds.end_logits[piece_index].detach().numpy()\n",
    "            offset_mapping = tokenized_data[\"offset_mapping\"][piece_index]\n",
    "            \n",
    "            best_start_tokens_ids = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            best_end_tokens_ids = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            \n",
    "            for start_token in best_start_tokens_ids:\n",
    "                for end_token in best_end_tokens_ids:\n",
    "                    if offset_mapping[start_token] is None or offset_mapping[end_token] is None:\n",
    "                        continue\n",
    "                    if (\n",
    "                            start_token > end_token or \n",
    "                            end_token - start_token + 1 > max_answer_len or \n",
    "                            end_token - start_token + 1 < min_answer_len\n",
    "                    ):\n",
    "                        continue\n",
    "                    valid_answers.append({\n",
    "                            \"score\": start_logits[start_token].item() + end_logits[end_token].item(),\n",
    "                            \"text\": context[offset_mapping[start_token][0] : offset_mapping[end_token][1]],\n",
    "                            \"id\": sample\n",
    "                        })\n",
    "            if not valid_answers:\n",
    "                valid_answers.append({\"score\": 0.0, \"text\": \"\", \"id\": sample})\n",
    "                \n",
    "        total_answers.append(sorted(valid_answers, key=lambda x: -x[\"score\"])[0]) # для всего контекста берем лучший по всем кусочкам\n",
    "    \n",
    "    return total_answers\n",
    "    \n",
    "some_samples = test_set[list(range(3))]\n",
    "inference(some_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T19:00:08.617701Z",
     "start_time": "2024-09-30T19:00:07.784200Z"
    }
   },
   "id": "b6f33c8f91e2940",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 663.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 3.2535845041275024, 'text': 'Никите Ляпину 20 лет', 'id': 0}]"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference({\"question\": [\"Сколько Никите Ляпину годиков?\"], \"context\": [\"Никите Ляпину 20 лет\"], \"id\": [0]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T19:00:14.203869Z",
     "start_time": "2024-09-30T19:00:14.158850Z"
    }
   },
   "id": "d55bb5355f2a5e0a",
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "source": [
    "Глазами видно, что модель назначает высочайшие скоры очень коротким ответам.\n",
    "Возможно я допустил багу в препроцессинге, надо посмотреть, как распределена установленная мной длина ответа в токенах.\n",
    "Возможно у нас есть проблемы с обучением."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1516f503f574a0fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сначала оценим-ка метрику"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b12edeb56fa600b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T18:58:48.410749Z",
     "start_time": "2024-09-30T18:58:46.773717Z"
    }
   },
   "id": "56f8b1784127c0f9",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preds_count = 1000\n",
    "predictions = inference(valid_set[:preds_count])\n",
    "formatted_predictions = [{\"id\": str(predic  tions[i][\"id\"]), \"prediction_text\": predictions[i][\"text\"]} for i in range(len(predictions))]\n",
    "references = [{\"id\": str(valid_set[i][\"id\"]), \"answers\": valid_set[i][\"answers\"]} for i in range(len(predictions))]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-09-30T20:18:53.810937Z"
    }
   },
   "id": "c288f6b99bbd6226",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '60544', 'prediction_text': 'В XXVII веке до н. э.'}"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_predictions[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T20:15:36.324966Z",
     "start_time": "2024-09-30T20:15:36.318987Z"
    }
   },
   "id": "62df6584bfe584fe",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '60544',\n 'answers': {'text': ['в Древнем Египте'], 'answer_start': [60]}}"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e57dcf85fb02259",
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'exact_match': 1.2, 'f1': 8.450913145631143}"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=formatted_predictions, references=references)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-30T20:16:42.937811Z",
     "start_time": "2024-09-30T20:16:41.813261Z"
    }
   },
   "id": "57bf68c94aca59fa",
   "execution_count": 108
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
